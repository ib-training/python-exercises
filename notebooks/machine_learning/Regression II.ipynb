{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weiteres zu Regressionen\n",
    "\n",
    "Im folgenden sollen noch weitere Aspekte zu Regressionen vorgestellt werden - zum einen werden wir jetzt mit mehr Features arbeiten, die Modelle werden komplexer (und eventuell auch zu komplex)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# einige Importe\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir beginnen erneut mit einem einfachen zufälligen Datensatz mit einem Feature X und Targets Y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "x = 2 - 3 * np.random.normal(0, 1, 20)\n",
    "y = x - 2 * (x ** 2) + 0.5 * (x ** 3) + np.random.normal(-3, 3, 20)\n",
    "\n",
    "# x wird noch in Spaltenform gebracht\n",
    "x = x[:, np.newaxis]\n",
    "\n",
    "# TODO: Scatterplot der Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein linearer Zusammenhang beschreibt die Daten nicht sehr gut - führen wir dennoch zunächst eine lineare Regression durch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Berechnen Sie ein lineares Modell mit einem SGDRegressor()\n",
    "model = ...\n",
    "\n",
    "y_pred = model.predict(x)\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, y_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statt einem linearen könnte hier auch ein polynomielles Modell genutzt werden. Unser Regressionsmodel ist allerdings linear - stattdessen fügen wir unserer Datenmatrix X noch weitere Spalten (Features) hinzu, in denen wir die Werte z.B. quadrieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_poly = np.c_[x, x**2]\n",
    "x_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Erzeugen Sie ein zweites Modell mit den erweiterten Features\n",
    "model2 = ...\n",
    "\n",
    "# TODO Daten plotten\n",
    "plt....\n",
    "\n",
    "\n",
    "# TODO die folgenden Zeilen plotten das Modell\n",
    "# x_axis = np.linspace(-5, 5, 20)\n",
    "# plt.plot(x_axis, model2.predict(np.c_[x_axis, x_axis**2]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In __sklearn__ ist das Erzeugen von polynomiellen Features bereits implementiert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features= PolynomialFeatures(degree=2)\n",
    "x_poly = polynomial_features.fit_transform(x)\n",
    "x_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nutzen wir dieses Feature, um ein Polynom vom Grad 10 zu fitten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Features bis zu Grad 10 erzeugen\n",
    "polynomial_features= ...\n",
    "x_poly = ...\n",
    "\n",
    "# hier nutzen wir einmal die sklearn-Implementierung ohne Gradientenabstieg\n",
    "# denn der ist bei Polynomen mit hohem Grad sehr instabil\n",
    "model3 = LinearRegression()\n",
    "model3.fit.... # TODO\n",
    "\n",
    "plt.... # TODO\n",
    "\n",
    "# TODO: die folgenden Zeilen plotten das Modell\n",
    "# x_axis = np.linspace(-5, 5, 100)[:, np.newaxis]\n",
    "# plt.plot(x_axis, model3.predict(polynomial_features.fit_transform(x_axis)));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir sehen - dies ist ein klassischer Fall von Overfitting. Das gewählte Model ist zu kompliziert. Es hat zwar einen sehr geringen Fehler (Datenpunkte liegen fast alle auf dem Polynomgraphen), aber die Vorhersagen dieses Models werden für x < -4 sehr unrealistisch.\n",
    "\n",
    "Ein Model, welches simplifiziert, und nicht sehr genau auf die Trainingsdaten anpasst, hat einen hohen _bias_ (wie das Model mit Grad 1).\n",
    "\n",
    "Ein Model, welches sehr variabel ist, und sich daher sehr genau an Trainingsdaten anpasst (wie das Modell mit Grad 10) hat eine sehr hohe _variance_.\n",
    "\n",
    "Beide Extreme führen zu schlechten Ergebnissen, das optimale Model liegt im Kompromiss dazwischen - man spricht daher vom _bias / variance tradeoff_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beispiel: Boston house prices\n",
    "\n",
    "Im folgenden wollen wir noch einige weitere Regressionen mit einem Beispieldatensatz rechnen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "import pandas as pd\n",
    "\n",
    "data = sklearn.datasets.load_boston()\n",
    "\n",
    "# TODO die folgende Zeile liefert etwas Dokumentation zum Datensatz\n",
    "# print(data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Untersuchen Sie die Variable data und  erzeugen ein Pandas dataframe\n",
    "# mit den Features\n",
    "df = ...\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO und die Zielwerte werden ebenfalls in Pandas verpacken\n",
    "df_target = ...\n",
    "df_target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Betrachten wir die Korrelationen der Features mit den Hauspreisen\n",
    "combined = pd.concat... # TODO Features und Preise zusammenfügen\n",
    "combined...             # TODO Korrelationen berechnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir splitten die Daten in Training und Testset, damit wir \n",
    "# unsere Modelle validieren können\n",
    "\n",
    "# TODO nutzen Sie die Funktion train_test_split \n",
    "# (80% für Training, 20% für Test)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = ...\n",
    "\n",
    "# TODO shapes ausgeben\n",
    "# print(X_train.shape)\n",
    "# print(X_test.shape)\n",
    "# print(Y_train.shape)\n",
    "# print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# für die lineare Regression mit Gradientenabstieg empfiehlt es sich,\n",
    "# die Daten zu standardisieren\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "sgd_regression_model = Pipeline([\n",
    "        # TODO erst standardisieren,\n",
    "        # TODO dann eine Regression rechnen\n",
    "])\n",
    "\n",
    "\n",
    "sgd_regression_model.fit(X_train, Y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# eine Hilfsmethode, die zwei Metriken ausgibt:\n",
    "# RMSE (root mean squared error)\n",
    "# r2 (Anteil der erklärten Varianz)\n",
    "def eval_model(model, X_train, X_test, Y_train, Y_test):\n",
    "    y_train_predict = model.predict(X_train)\n",
    "    rmse = (np.sqrt(mean_squared_error(Y_train, y_train_predict)))\n",
    "    r2 = r2_score(Y_train, y_train_predict)\n",
    "\n",
    "    print(\"Model performance (training)\")\n",
    "    print(\"--------------------------------------\")\n",
    "    print('RMSE is {}'.format(rmse))\n",
    "    print('R2 score is {}'.format(r2))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # model evaluation for testing set\n",
    "    y_test_predict = model.predict(X_test)\n",
    "    rmse = (np.sqrt(mean_squared_error(Y_test, y_test_predict)))\n",
    "    r2 = r2_score(Y_test, y_test_predict)\n",
    "\n",
    "    print(\"Model performance (test)\")\n",
    "    print(\"--------------------------------------\")\n",
    "    print('RMSE is {}'.format(rmse))\n",
    "    print('R2 score is {}'.format(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluieren wir das erste Model\n",
    "eval_model(...) # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zum Vergleich trainieren wir noch einen RandomForestRegressor\n",
    "# mit 1000 estimators\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = ... # TODO Objekt erzeugen\n",
    "rf...    # TODO Model berechnen\n",
    "\n",
    "# TODO Model evaluieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... und noch einmal mit GradientBoosting\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbr = ... # TODO Objekt erzeugen\n",
    "gbr...    # TODO Model berechnen\n",
    "\n",
    "# TODO Model evaluieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus: Wir können betrachten, welche Features beim Gradient-Boosting\n",
    "# wie wichtig sind\n",
    "plt.plot(range(len(gbr.feature_importances_)), gbr.feature_importances_)\n",
    "\n",
    "# TODO Passt die Ausgabe zu den Korrelationen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus 2: Wir plotten einmal die Daten, und die \n",
    "# Vorhersagen des Gradient-Boosting-Modells\n",
    "# Achse 1: wichtigstes Feature\n",
    "# Achse 2: zweitwichtigstes Feature\n",
    "# Achse 3: Hauspreis\n",
    "\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlabel('TODO')\n",
    "ax.set_ylabel('TODO')\n",
    "ax.set_zlabel('TODO')\n",
    "\n",
    "# TODO\n",
    "ax.scatter(data.data[...], data.data[...], data.target)\n",
    "ax.scatter(data.data[...], data.data[...], gbr.predict(data.data))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
