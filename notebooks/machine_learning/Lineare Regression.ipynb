{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lineare Regression\n",
    "\n",
    "In diesem Notebook werden wir eine lineare Regression selbst implementieren, und dabei Numpy nutzen. Im späteren Verlauf werden wir dann __sklearn__ verwenden. Das Beispiel soll darüber hinaus einige Grundbegriffe des maschinellen Lernens vermitteln."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard-Imports zum Arbeiten mit Numpy\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir beginnen zunächst mit einem einfachen zufälligen Datensatz mit einem Feature X und Targets Y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42) # fixierter Seed-Wert\n",
    "x = 10 * rng.rand(50) \n",
    "y = 2*x -1 + rng.randn(50) \n",
    "\n",
    "# TODO plotten Sie die Daten als Scatterplot:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ziel ist es, eine Regressionsgerade zu finden, also ein lineares Modell, welches den Zusammenhang zwischen x und y möglichst gut beschreibt. Unser Modell hat die folgende Form:\n",
    "\n",
    "$$ \\hat{y}  = \\beta_0 + \\beta_1 x $$\n",
    "\n",
    "$\\beta_0$ ist dabei ein konstanter Anteil (genannt __bias__), in unserem einfachen Fall der y-Achsenabschnitt, und $\\beta_1$ ist die Steigung unserer Geraden. $\\hat{y}$ wird oft auch _prediction_ genannt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_betas = np.array([-1, 2])\n",
    "bad_betas = np.array([3, 1.2])\n",
    "\n",
    "# TODO plotten Sie die beiden Geraden, die durch die gegebenen beta-Koeffizienten entstehen:\n",
    "# y_pred_good = ...\n",
    "# y_pred_bad = ...\n",
    "\n",
    "# plt.scatter(...);\n",
    "# plt.plot(x, ...)\n",
    "# plt.plot(x, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn wir unsere x-Werte als Matrix organisieren, und als erste Spalte eine Spalte mit Einsen hinzufügen, können wir diese Berechnung als Matrixoperation ausdrücken (und später auf beliebig viele Features erweitern):\n",
    "\n",
    "$$ \\hat{y} = X\\beta $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[np.ones(len(x)), x] # np._c[...] ist eine Kurzschreibweise, \n",
    "                              # die es erlaubt, mehrere Spalten zu einer Matrix zusammenzufügen\n",
    "\n",
    "# TODO zeigen Sie die ersten 5 Zeilen von X an\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_good = ...\n",
    "y_pred_bad = ...\n",
    "\n",
    "# TODO und erneut plotten - kann man von oben kopieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt definieren wir eine Fehlerfunktion $J$ (_loss function_ oder _cost function_ ), die die quadratischen Fehler/Abweichungen der Datenpunkte von einer Geraden berechnet ($X$ - Features, $y$ - Targets, $\\hat{y}$ - Vorhersagen des Models):\n",
    "\n",
    "$$ J(\\beta) = \\frac{1}{2m} \\sum_{j=1}^m{(y^j - \\hat{y}^j)^2} $$\n",
    "\n",
    "oder vektoriell geschrieben:\n",
    "\n",
    "$$J(\\beta) = \\frac{1}{2m} (y - \\hat{y})^T (y - \\hat{y})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implementieren Sie die Fehlerfunktion \"cost\" mit numpy:\n",
    "\n",
    "def cost(y, y_pred):\n",
    "    pass\n",
    "\n",
    "# Tests\n",
    "# assert(math.isclose(cost(y, X @ good_betas), 0.41358136117306665))\n",
    "# assert(math.isclose(cost(y, X @ bad_betas), 2.979345185814611))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die besten Parameter $\\beta$ zu finden, wird jetzt die Fehlerfunktion nach $\\beta$ abgeleitet. Im Falle der linearen Regression kann man sogar direkt eine Lösung angeben, im Allgemeinen wird man die Ableitung (den Gradienten) nutzen, um mit einem Gradientenabstieg ein (eventuell lokales) Minimum zu finden. Lösung (die Herleitung sprengt hier unseren Rahmen):\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial \\beta} =  \\frac{1}{m} ( X^T (\\hat{y} - y)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO implementieren Sie die Funktion gradient mit numpy:\n",
    "def gradient(y, y_pred, X):\n",
    "    pass\n",
    "\n",
    "# assert(np.allclose(gradient(y, y_pred_good, X), np.array([0.00294529, 0.19586796])))\n",
    "# assert(np.allclose(gradient(y, y_pred_bad, X), np.array([ 0.43555405, -4.41778084])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um per Gradientenabstieg den Fehler zu minimieren, können wir mit zufälligen Parametern $\\beta^0$ beginnen - dann berechnen wir den Gradienten der Fehlerfunktion für diese Parameter, und gehen dann im Parameterraum einen kleinen Schritt in Richtung des stärksten Abstiegs (der Gradient zeigt in Richtung der stärksten Steigung im Parameterraum) - die Update-Regel lautet:\n",
    "\n",
    "$$ \\beta \\leftarrow \\beta - \\textit{lr} \\frac{\\partial J}{\\partial \\beta} (\\beta)$$\n",
    "\n",
    "($\\textit{lr}$ ist dabei die _Lernrate_ - z.B. 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradientenabstieg\n",
    "\n",
    "# Start mit einem beliebigen Wert für beta\n",
    "beta = np.array([0.0, 0.0])\n",
    "steps = 20\n",
    "lr = 0.01\n",
    "\n",
    "costs = np.zeros(steps)\n",
    "betas = np.zeros((steps, 2))\n",
    "\n",
    "for i in range(steps):\n",
    "    y_pred = ...      # TODO\n",
    "    costs[i] = ...    # Fehler aufzeichnen\n",
    "    betas[...] = ...  # Parameter aufzeichnen\n",
    "    beta = ...        # neues beta berechnen (Update-Regel)\n",
    "\n",
    "# zum Plotten\n",
    "# plt.figure()                 \n",
    "# plt.plot(costs)\n",
    "# plt.title(\"Fehlerentwicklung\")\n",
    "\n",
    "# plt.figure()\n",
    "# plt.title(\"Parameterraum\")\n",
    "# plt.plot(betas[:,0], betas[:,1], '.-')\n",
    "\n",
    "# plt.figure()\n",
    "# plt.title(\"Entwicklung der Regressionsgeraden\")\n",
    "# plt.scatter(x, y)\n",
    "\n",
    "# for i in range(steps):\n",
    "#     y_pred = X @ betas[i, :]\n",
    "#     plt.plot(x, y_pred, 'g')\n",
    "\n",
    "# print(\"Betas:\", beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lineare Regression objektorientiert\n",
    "\n",
    "Im folgenden soll der entwickelte Code noch in eine Klasse verpackt werden - kopieren Sie Ihre Methoden an die geeigneten Stellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionGradientDescent:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def cost(self, y, y_pred):\n",
    "        pass # TODO\n",
    "\n",
    "    def gradient(self, y, y_pred, X):\n",
    "        pass # TODO\n",
    "        \n",
    "    def predict(self, X):\n",
    "        pass # TODO\n",
    "        \n",
    "    def fit(self, X, y, steps=20):\n",
    "        pass # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test - so wird die Klasse verwendet\n",
    "lin_model = LinearRegressionGradientDescent()\n",
    "lin_model.fit(X, y)\n",
    "y_pred = lin_model.predict(X)\n",
    "\n",
    "plt.scatter(X[:, 1], y)\n",
    "plt.plot(X[:, 1], y_pred)\n",
    "\n",
    "print(\"Betas\", lin_model.beta)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lin_model.costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In __sklearn__ ist dieses Verfahren (allerdings mit stochastischem Gradientenabstieg und Regularisierung) bereits implementiert, und die Schnittstelle ähnelt unserer Klasse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eine ähnliche Berechnung mit sklearn\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "lin_model = SGDRegressor(max_iter=100, eta0=lr)\n",
    "lin_model.fit(X, y)\n",
    "y_pred = lin_model.predict(X)\n",
    "\n",
    "plt.scatter(X[:, 1], y)\n",
    "plt.plot(X[:, 1], y_pred)\n",
    "\n",
    "print(\"Betas\", lin_model.coef_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
